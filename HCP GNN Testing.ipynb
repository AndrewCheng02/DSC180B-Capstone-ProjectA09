{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Imports\n",
    "# #Dataprep\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from nilearn import datasets, plotting, image\n",
    "# from nilearn.maskers import NiftiMapsMasker\n",
    "# from sklearn.decomposition import FastICA\n",
    "# import pandas as pd\n",
    "# import tarfile\n",
    "# import gzip\n",
    "# #sim required\n",
    "# import random\n",
    "# import pingouin as pg\n",
    "# import seaborn as sns\n",
    "# #GNN required\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn import Linear\n",
    "# import torch.nn as nn\n",
    "# import torch_geometric\n",
    "# from torch_geometric.nn import GCNConv\n",
    "# from torch_geometric.nn import SAGEConv\n",
    "# from torch_geometric.nn import GraphConv\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from torch.utils.data import SubsetRandomSampler\n",
    "# from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1. Input Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverables:**  \n",
    "**Goal**: Create different sub-graphs two decrease training times and aid in overall interpretability of GNN performance\n",
    "\n",
    "**Sub-graph Types**\n",
    "1. Significant Regions [alpha] \n",
    "2. Significant Regions [bonferroni-corrected] \n",
    "3. Graph Sampling [GRAPHSaint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jense\\AppData\\Local\\Temp\\ipykernel_14472\\2238771519.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# imports for this section\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Read in the HCP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>netmat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100206</th>\n",
       "      <td>M</td>\n",
       "      <td>26-30</td>\n",
       "      <td>[[0.0, 0.61676, 9.5727, -5.4959, 0.34639, 3.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100307</th>\n",
       "      <td>F</td>\n",
       "      <td>26-30</td>\n",
       "      <td>[[0.0, -0.29664, 17.317, -9.0467, -0.28723, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100408</th>\n",
       "      <td>M</td>\n",
       "      <td>31-35</td>\n",
       "      <td>[[0.0, 1.6486, 6.6189, -8.8877, 1.4337, 1.006,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100610</th>\n",
       "      <td>M</td>\n",
       "      <td>26-30</td>\n",
       "      <td>[[0.0, -0.90275, 7.7215, -8.3907, 3.3144, 2.93...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101006</th>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>[[0.0, -0.088768, 9.4979, -10.412, 1.0646, 4.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Gender    Age                                             netmat\n",
       "subject_id                                                                 \n",
       "100206          M  26-30  [[0.0, 0.61676, 9.5727, -5.4959, 0.34639, 3.00...\n",
       "100307          F  26-30  [[0.0, -0.29664, 17.317, -9.0467, -0.28723, 1....\n",
       "100408          M  31-35  [[0.0, 1.6486, 6.6189, -8.8877, 1.4337, 1.006,...\n",
       "100610          M  26-30  [[0.0, -0.90275, 7.7215, -8.3907, 3.3144, 2.93...\n",
       "101006          F  31-35  [[0.0, -0.088768, 9.4979, -10.412, 1.0646, 4.3..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'Data/data_clean.csv.gz'\n",
    "\n",
    "with gzip.open(data_file) as filepath:\n",
    "    data = pd.read_csv(filepath, index_col = 'subject_id', \n",
    "            converters = {'netmat' : lambda x : np.array(literal_eval(re.sub('(?<!\\[)\\s+|[\\\\n]', ', ', x)))})\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Get the input subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: INPUT DATA TYPES\n",
    "# -> FILTERING\n",
    "# 1. Sig Regions [alpha] \n",
    "# 2. Sig Regions [bonferroni-corrected] \n",
    "# 3. Graph Sampling [GRAPHSaint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 297 ms\n",
      "Wall time: 298 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# full dataset\n",
    "graphs = np.stack(data.netmat.to_numpy()) # all 1003 correlation matrices\n",
    "edges = np.argwhere(graphs)\n",
    "# significant edges\n",
    "edges_sig = np.triu(np.genfromtxt('significant_edges.csv', delimiter=',')) # only keep unique edges\n",
    "graphs_sig = np.where(edges_sig, graphs, 0) # only keep significant correlations\n",
    "# bonferroni significant edges\n",
    "edges_sig_b = np.triu(np.genfromtxt('bonferroni_sig_edges.csv', delimiter=',')) # only keep unique edges\n",
    "graphs_sig_b = np.where(edges_sig, graphs, 0)# only keep significant correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get edges into correct format for later\n",
    "edges_sig = torch.tensor(np.argwhere(edges_sig).T)\n",
    "edges_sig_b = torch.tensor(np.argwhere(edges_sig_b).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jense\\AppData\\Local\\Temp\\ipykernel_14472\\1997748793.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['Gender'] = data['Gender'].replace({\"M\":0, \"F\":1})\n"
     ]
    }
   ],
   "source": [
    "# labels\n",
    "data['Gender'] = data['Gender'].replace({\"M\":0, \"F\":1})\n",
    "labels = torch.tensor(data['Gender'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0,  ..., 1, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Train-Validate-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for this section\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data_list):\n",
    "    \"\"\"\n",
    "    Unpacks graphs and their corresponding labels from a list of tuples.\n",
    "\n",
    "    Parameters:\n",
    "    - data_list (list of tuples): A list where each tuple contains a graph and its corresponding label.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary with the following keys:\n",
    "    - 'graph' (list): A list containing all the graphs extracted from the tuples.\n",
    "    - 'label' (list): A list containing all the labels extracted from the tuples.\n",
    "    \"\"\"\n",
    "    graphs, labels = map(list, zip(*data_list))\n",
    "    return {\n",
    "        'graph': graphs,\n",
    "        'label': labels\n",
    "    }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrelationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, netmats, edges, labels, split_ratio=(0.8, 0.1, 0.1)):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with pairwise correlation matrices, labels, and edges.\n",
    "    \n",
    "        Parameters:\n",
    "        - netmats (array-like): Pairwise correlation matrices.\n",
    "        - edges (array-like): Regions that make up each pairwise correlation from `netmats`.\n",
    "        - labels (array-like): Labels indicating gender (0 for male, 1 for female).\n",
    "        - split_ratio (tuple): Ratios for splitting the data into train, validation, and test sets. Default is (0.8, 0.1, 0.1).\n",
    "    \n",
    "        Splits the data into training, validation, and test sets based on the specified split ratios.\n",
    "        \"\"\"\n",
    "        self.netmats = netmats # pairwise correlation matrix\n",
    "        self.labels = labels # female (1) or male (0)\n",
    "        self.edges = edges # regions that make up each pairwise correlation from `netmats`\n",
    "        self.split_ratios = split_ratio\n",
    "        tot = len(self.netmats)\n",
    "        train_samples = int(split_ratio[0] * tot) # 0.8\n",
    "        val_samples = int(split_ratio[1] * tot) # 0.1\n",
    "\n",
    "        # splitting the data\n",
    "        self.train_indices = np.arange(0, train_samples) # training data\n",
    "        self.val_indices = np.arange(train_samples, train_samples + val_samples) # validation data\n",
    "        self.test_indices = np.arange(train_samples + val_samples, tot) # test data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of entire dataset (train + validate + test)\n",
    "        \"\"\"\n",
    "        return len(self.netmats)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the data and label corresponding to the given index.\n",
    "    \n",
    "        Parameters:\n",
    "        - idx (int): Index of the data sample to retrieve.\n",
    "    \n",
    "        Returns:\n",
    "        - graph_data (torch_geometric.data.Data): Graph data containing the correlation matrix and edge indices.\n",
    "        - label (int): Label indicating the gender (0 for male, 1 for female) of the corresponding data sample.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(self.netmats[idx]).float() # correlation matrix\n",
    "        edge_index = self.edges # dependent on input subgraph\n",
    "        graph_data = Data(x=x, edge_index=edge_index) \n",
    "        label = self.labels[idx]\n",
    "        return graph_data, label\n",
    "\n",
    "    def get_split(self, idx):\n",
    "        \"\"\"\n",
    "        Determines the split of a data sample based on its index.\n",
    "    \n",
    "        Parameters:\n",
    "        - idx (int): Index of the data sample.\n",
    "    \n",
    "        Returns:\n",
    "        - split (str): The split of the data sample ('train', 'val', or 'test').\n",
    "    \n",
    "        Raises:\n",
    "        - ValueError: If the index is not found in any of the splits.\n",
    "        \"\"\"\n",
    "        if idx in self.train_indices:\n",
    "            return 'train'\n",
    "        elif idx in self.val_indices:\n",
    "            return 'val'\n",
    "        elif idx in self.test_indices:\n",
    "            return 'test'\n",
    "        else:\n",
    "            raise ValueError('Index not in any split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(netmats, edges, labels=labels, collate_fn=collate):\n",
    "    \"\"\"\n",
    "    Creates a dataset and corresponding data loaders for training, validation, and testing.\n",
    "\n",
    "    Parameters:\n",
    "    - netmats (array-like): Pairwise correlation matrices.\n",
    "    - edges (array-like): Regions that make up each pairwise correlation from `netmats`.\n",
    "    - labels (array-like): Labels indicating gender (0 for male, 1 for female).\n",
    "    - collate_fn (function, optional): Function to collate data samples into batches. Default is `collate`.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the following keys:\n",
    "    - 'dataset' (CorrelationDataset): The created dataset.\n",
    "    - 'train_loader' (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "    - 'validate_loader' (torch.utils.data.DataLoader): DataLoader for validation data.\n",
    "    - 'test_loader' (torch.utils.data.DataLoader): DataLoader for test data.\n",
    "    \"\"\"\n",
    "    dataset = CorrelationDataset(netmats, edges, labels, split_ratio=(0.8, 0.1, 0.1))\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=SubsetRandomSampler(dataset.train_indices), collate_fn=collate_fn)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=SubsetRandomSampler(dataset.val_indices), collate_fn=collate_fn)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=SubsetRandomSampler(dataset.test_indices), collate_fn=collate_fn)\n",
    "    return {\n",
    "        'dataset': dataset,\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train-validate-test splits\n",
    "# full dataset\n",
    "# full =  create_dataset(graphs, edges.T)\n",
    "# significant pairwise correlations only\n",
    "sig = create_dataset(graphs_sig, edges_sig)\n",
    "# bonferroni significant pairwise correlations only\n",
    "# sig_b = create_dataset(graphs_sig_b, np.argwhere(edges_sig_b).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph': [Data(x=[100, 100], edge_index=[2, 1695])], 'label': [tensor(1)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = iter(sig['train_loader'])\n",
    "next(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: MODEL DEVELOPMENT - VIEW EMBEDDING SPACES BEFORE TRAINING\n",
    "# STEP 3: MODEL SELECTION/FEATURE SELECTION\n",
    "# we will test many different combinations of inputs and architectures\n",
    "# INPUT TYPES\n",
    "# 1. 10 regions [450 male subjects, 550 female subjects (1000 total netmats)]\n",
    "# 2. 20 regions [450 male subjects, 550 female subjects (1000 total netmats)]\n",
    "# 3. 50 regions [450 male subjects, 550 female subjects (1000 total netmats)]\n",
    "# 4. 100 regions [450 male subjects, 550 female subjects (1000 total netmats)]\n",
    "#\n",
    "# ARCHITECTURE TYPES:\n",
    "# -> layers\n",
    "# 1. 1-layer\n",
    "# 2. 2-layer\n",
    "# 3. 3-layer\n",
    "# 4. 4-layer\n",
    "# -> squishificaton function (mapping to [-1,1])\n",
    "# 1. tanh\n",
    "# 2. ReLu\n",
    "# 3. sigmoid\n",
    "# -> embedding space dimension:\n",
    "# 1. 50\n",
    "# 2. 100\n",
    "# 3. 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# og\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(100, 150)\n",
    "        self.bn1 = nn.BatchNorm1d(150)\n",
    "        self.conv2 = GCNConv(150, 50)\n",
    "        self.bn2 = nn.BatchNorm1d(50)\n",
    "        self.classifier = Linear(50, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = data['graph'][0]\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = nn.Dropout(p=0.75)(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = nn.Dropout(p=0.75)(x)\n",
    "        x = self.classifier(x).mean(dim=0)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(100, 150)\n",
      "  (bn1): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GCNConv(150, 50)\n",
      "  (bn2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (classifier): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, device, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_true = 0\n",
    "    num_true_guesses = 0\n",
    "    for d in data_loader:\n",
    "        m = d['graph'][0]\n",
    "        m = m.to(device)\n",
    "        label = d['label'][0]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(d).squeeze()\n",
    "        loss = loss_fn(out.float(), label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = (out.double() > 0.5).float()\n",
    "        if pred == label.double():\n",
    "            num_true += 1\n",
    "        if pred == 1:\n",
    "            num_true_guesses += 1\n",
    "    return total_loss, num_true, num_true_guesses\n",
    "\n",
    "def eval(model, loader, device):\n",
    "    model.eval()\n",
    "    cor = 0\n",
    "    tot = 0\n",
    "    for d in loader:\n",
    "        with torch.no_grad():\n",
    "            out = model(d)\n",
    "            pred = (out.double() > 0.5).float()\n",
    "        y = d['label'][0]\n",
    "        cor += (pred == y).sum()\n",
    "        tot += pred.shape[0]\n",
    "    return cor/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'train_loader'\n",
    "val_data = 'val_loader'\n",
    "test_data = 'test_loader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Loss: 556.6974675953388, Avg Loss: 0.694, Train: 55.74%, Validation: 59.00%, Num Correct: 440, True Guesses: 624 \n",
      "Epoch: 2/50, Loss: 543.309790790081, Avg Loss: 0.677, Train: 56.36%, Validation: 58.00%, Num Correct: 466, True Guesses: 488 \n",
      "Epoch: 3/50, Loss: 513.3446044176817, Avg Loss: 0.640, Train: 54.61%, Validation: 54.00%, Num Correct: 487, True Guesses: 477 \n",
      "Epoch: 4/50, Loss: 454.71947017125785, Avg Loss: 0.567, Train: 56.11%, Validation: 51.00%, Num Correct: 565, True Guesses: 461 \n",
      "Epoch: 5/50, Loss: 444.47852848796174, Avg Loss: 0.554, Train: 54.24%, Validation: 47.00%, Num Correct: 590, True Guesses: 454 \n",
      "Epoch: 6/50, Loss: 403.6643788162619, Avg Loss: 0.503, Train: 57.98%, Validation: 47.00%, Num Correct: 601, True Guesses: 435 \n",
      "Epoch: 7/50, Loss: 393.21853377181105, Avg Loss: 0.490, Train: 58.35%, Validation: 53.00%, Num Correct: 609, True Guesses: 451 \n",
      "Epoch: 8/50, Loss: 386.9974483246915, Avg Loss: 0.483, Train: 56.86%, Validation: 57.00%, Num Correct: 622, True Guesses: 452 \n",
      "Epoch: 9/50, Loss: 376.20546450512484, Avg Loss: 0.469, Train: 56.86%, Validation: 49.00%, Num Correct: 628, True Guesses: 434 \n",
      "Epoch: 10/50, Loss: 355.8150246277364, Avg Loss: 0.444, Train: 56.73%, Validation: 47.00%, Num Correct: 636, True Guesses: 440 \n",
      "Epoch: 11/50, Loss: 353.39450198970735, Avg Loss: 0.441, Train: 54.49%, Validation: 46.00%, Num Correct: 632, True Guesses: 428 \n",
      "Epoch: 12/50, Loss: 339.15021441760473, Avg Loss: 0.423, Train: 54.49%, Validation: 46.00%, Num Correct: 650, True Guesses: 434 \n",
      "Epoch: 13/50, Loss: 322.30277660366846, Avg Loss: 0.402, Train: 54.61%, Validation: 48.00%, Num Correct: 653, True Guesses: 441 \n",
      "Epoch: 14/50, Loss: 330.6351901599555, Avg Loss: 0.412, Train: 53.49%, Validation: 50.00%, Num Correct: 650, True Guesses: 424 \n",
      "Epoch: 15/50, Loss: 308.18509488133714, Avg Loss: 0.384, Train: 55.24%, Validation: 45.00%, Num Correct: 667, True Guesses: 439 \n",
      "Epoch: 16/50, Loss: 305.52338947780663, Avg Loss: 0.381, Train: 53.49%, Validation: 53.00%, Num Correct: 660, True Guesses: 434 \n",
      "Epoch: 17/50, Loss: 308.7584473339375, Avg Loss: 0.385, Train: 57.48%, Validation: 49.00%, Num Correct: 666, True Guesses: 444 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:12\u001b[0m\n",
      "Cell \u001b[1;32mIn[18], line 29\u001b[0m, in \u001b[0;36meval\u001b[1;34m(model, loader, device)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 29\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(d)\n\u001b[0;32m     30\u001b[0m         pred \u001b[38;5;241m=\u001b[39m (out\u001b[38;5;241m.\u001b[39mdouble() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     31\u001b[0m     y \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(x)\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(x)\n\u001b[1;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m)(x)\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\capstone\\Lib\\site-packages\\torch\\nn\\functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GCN().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
    "criterion = nn.BCELoss()\n",
    "losses = []\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    loss, num_true, num_true_guesses = train(model, sig[train_data], device, criterion, optimizer)\n",
    "    train_results = eval(model, sig[train_data], device)\n",
    "    val_results = eval(model, sig[val_data], device)\n",
    "    losses.append(loss)\n",
    "    avg_loss = loss / len(sig[train_data])\n",
    "    print(f'Epoch: {epoch + 1}/{num_epochs}, '\n",
    "          f'Loss: {loss}, '\n",
    "          f'Avg Loss: {avg_loss:.3f}, '\n",
    "          f'Train: {100 * train_results:.2f}%, '\n",
    "          f'Validation: {100 * val_results:.2f}%, '\n",
    "          f'Num Correct: {num_true}, '\n",
    "          f'True Guesses: {num_true_guesses} ')\n",
    "    \n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
